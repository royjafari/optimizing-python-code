{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "637d998f",
   "metadata": {},
   "source": [
    "# Optimizing the Python Code for Big Data \n",
    "Balancing Coding Complexity against Computational Complexity \n",
    "\n",
    "    \n",
    "    AUTHOR: Dr. Roy Jafari \n",
    "\n",
    "# Chapter 5: Picking up the right tool \n",
    "\n",
    "## Challenge 3: Restructuring and Reformulating Data â€“ Second Case Study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a7f57ca",
   "metadata": {},
   "source": [
    "In this challenge, we address a common issue in data preparation. Often, data is stored in formats that either omit zero-value objects or use lists or dictionaries within individual table cells to save disk space. While this method is efficient for storage, it can significantly complicate data preparation. This learning opportunity allows us to tackle these challenges and explore optimized tools for handling them. Follow the nine steps below to familiarize yourself with these challenges and learn how to manage them effectively.\n",
    "\n",
    "1. The code below uses `pd.read_csv()` to load the `stock_news.csv` file into the `news_df` DataFrame. Execute the code and review the dataset:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "news_df = pd.read_csv('stock_news.csv')\n",
    "news_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e42a4-2e6e-4e62-84fa-848999ba9df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a126179-1a4a-409d-aadb-caa543e58e0b",
   "metadata": {},
   "source": [
    "2. The following code confirms that this data belongs to the year 2023.\n",
    "\n",
    "```python\n",
    "news_df.DateTime = pd.to_datetime(news_df['DateTime'])\n",
    "print(news_df.DateTime.min())\n",
    "print(news_df.DateTime.max())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ac67d-76de-4764-b3a9-05d96e7532cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a54812f0-cbff-413d-80dc-c85a85e8348e",
   "metadata": {},
   "source": [
    "3. Write a code snippet that extracts a unique list of all the stocks mentioned in the 'Entity' column of your dataframe. Challenge yourself to use the most efficient tools and techniques you can think of. Do not proceed to the next steps until you have completed this task. Once you are done, I will demonstrate the optimal tools and methods you could have used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e95f52-3bf0-4e0b-9f79-fe2e539e1eef",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45cff3-47fc-4bd4-b8c4-692c389688c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5d3e12-8b30-4eaf-9d72-5e49342d69bb",
   "metadata": {},
   "source": [
    "4. The code below addresses the challenge described in Step 3, utilizing the following tools: the `.str` accessor, `.replace()`, `.split()`, and `.explode()` functions from pandas, along with Python's `set()` and `list()` functions. Execute the code and compare its runtime with the method you previously implemented.\n",
    "\n",
    "```python\n",
    "all_stocks = list(\n",
    "    set(\n",
    "        news_df.Entities\n",
    "        .str[1:-1]\n",
    "        .str.replace(\"'\", \"\")\n",
    "        .str.replace(\" \", \"\")\n",
    "        .str.split(',')\n",
    "        .explode()\n",
    "        .values.tolist()\n",
    "    )\n",
    ")\n",
    "print(f\"There are {len(all_stocks)} different stocks in news_df.\")\n",
    "all_stocks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd6084-e6f8-43b6-8393-38bcca9c511b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d005332d-4381-471d-9121-aa14c2a2c10a",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908d304-71f6-404f-9362-67a8dab841a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8145dab-f8d0-4792-916b-8cbe1056f9e9",
   "metadata": {},
   "source": [
    "5. Each row of `news_df` represents one stock news. We want to use this dataset to create `sentiment_df`, where each row will represent the aggregate sentiment scores of each stock for each hour. The following code creates a Pandas DataFrame that is an empty version of `sentiment_df`. Run the following code to create `sentiment_df` and study its structure.\n",
    "\n",
    "```python\n",
    "import datetime\n",
    "\n",
    "# Ensure the DateTime column is in datetime format\n",
    "news_df['DateTime'] = pd.to_datetime(news_df['DateTime'])\n",
    "\n",
    "# Generate all hours for the year 2023\n",
    "all_hours = [\n",
    "    datetime.datetime(2023, 1, 1) + datetime.timedelta(hours=i) \n",
    "    for i in range(365 * 24)]\n",
    "\n",
    "# Create a MultiIndex using the list of stocks and all hours\n",
    "my_multi_index = pd.MultiIndex.from_product(\n",
    "    (all_stocks, all_hours), \n",
    "    names=['Ticker', 'DateTime'])\n",
    "\n",
    "# Create an empty DataFrame with the specified MultiIndex and columns for sentiment scores\n",
    "sentiment_df = pd.DataFrame(\n",
    "    index=my_multi_index, \n",
    "    columns=['Positive', 'Negative', 'Neutral','n_news'])\n",
    "\n",
    "# Display the structure of the empty DataFrame\n",
    "sentiment_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc5ed6-3153-49e5-82f2-b3d4b7a07156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8749673d-5865-4c82-b7c0-1bd1f280cc94",
   "metadata": {},
   "source": [
    "6. Your challenge in this step is to create the complete version of the `sentiment_df` DataFrame. Ensure you choose the most efficient tools and techniques to accomplish this task. Pay attention to whether you've picked the best tools to get this done. Once you are done, I will demonstrate the optimal tools and methods you could have used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf33c0a5-3641-4d5a-8c92-0f451ff64162",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd80a85-eeab-4989-a6c7-90d5a5b65e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "728cfd4c-4408-482a-9539-96dff1807560",
   "metadata": {},
   "source": [
    "7. The following code completes the challenge described in the previous step by utilizing the `.explode()` and `.groupby()` functions in an optimized way. Compare the runtime of your own code with this optimized version:\n",
    "```python\n",
    "%%time\n",
    "news_df['Ticker'] = (news_df.Entities\n",
    "        .str[1:-1]\n",
    "        .str.replace(\"'\", \"\")\n",
    "        .str.replace(\" \", \"\")\n",
    "        .str.split(','))\n",
    "sentiment_df = (news_df\n",
    "                .explode('Ticker')\n",
    "                .groupby(['Ticker', 'DateTime_Hour'])\n",
    "                [['Positive', 'Negative', 'Neutral']]\n",
    "                .mean()\n",
    "            )\n",
    "sentiment_df['n_news'] = (news_df\n",
    "                .explode('Ticker')\n",
    "                .groupby(['Ticker', 'DateTime_Hour'])\n",
    "                .size()\n",
    "            )\n",
    "sentiment_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669ece7-213d-4364-86b3-96427df9c4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eccb2c4-f9c5-4d11-b2b9-a7adb32b89dd",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afeb86-67d9-4455-ad76-7c89ec21b68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e630a4aa-70ab-4ea6-b02c-ab907fb497d8",
   "metadata": {},
   "source": [
    "8. The `sentiment_df` we get from the previous step does not have the exact data structure as the one described in step 5. Essentially, if during an hour we don't have any news for a ticker, the rows for that ticker have been omitted. Use the script that leverages what we completed in step 6, to ensure our `sentiment_df` will be complete. Pay attention to whether you've picked the best tools to get this done. Once you are done, I will demonstrate the optimal tools and methods you could have used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0954257-8b80-42c8-bc34-de2ae2b0aaff",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb46ed-4828-4a89-b023-a51ad7c2c166",
   "metadata": {},
   "source": [
    "9. The following code snippet leverages the `.update()` function to address the challenge described in the previous step in an optimized manner. Please study and execute the code, then compare its runtime with your previous implementation:\n",
    "\n",
    "```python\n",
    "news_df['DateTime'] = pd.to_datetime(news_df['DateTime'])\n",
    "all_hours = [\n",
    "    datetime.datetime(2023, 1, 1) + \n",
    "    datetime.timedelta(hours=i) for i in range(365 * 24)]\n",
    "my_multi_index = pd.MultiIndex.from_product(\n",
    "    [all_stocks, all_hours], \n",
    "    names=['Ticker', 'DateTime_Hour'])\n",
    "stage_df = pd.DataFrame(\n",
    "    index=my_multi_index, \n",
    "    columns=['Positive', 'Negative', 'Neutral', 'n_news'])\n",
    "stage_df['n_news'] = 0.0\n",
    "\n",
    "# Update the DataFrame with values from sentiment_df\n",
    "stage_df.update(sentiment_df)\n",
    "\n",
    "# Copy the updated DataFrame to sentiment_df for further use\n",
    "sentiment_df = stage_df.copy()\n",
    "\n",
    "# Output the updated DataFrame\n",
    "sentiment_df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e079a-fc14-4790-80d2-ba8a2dd5551e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf420057-053c-4395-be7f-f7aeb61187d9",
   "metadata": {},
   "source": [
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f42736-3d87-4a56-a5fb-0255f22d2b8a",
   "metadata": {},
   "source": [
    "10. For your future reference, it's useful to note the situations and tools from this challenge where optimization is most effective. The tools we used are `.explode()`, `pd.MultiIndex` and `.update()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469d3a5-a917-4803-af7a-68bf6cbe56b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a57c402-4ae1-4bb0-8735-dca54abcbe87",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
